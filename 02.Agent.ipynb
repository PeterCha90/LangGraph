{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "LangGraph_agent_101\n"
     ]
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()\n",
    "\n",
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"LangGraph_agent_101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8) - 테디노트',\n",
       "  'url': 'https://teddylee777.github.io/langchain/langchain-tutorial-08/',\n",
       "  'content': '⑥ 테스트\\n태그:\\nChatGPT,\\nChatOpenAI,\\nGPT3.5,\\nGPT4,\\nlangchain,\\nlangchain tutorial,\\nOpenAI,\\nPDF,\\n랭체인,\\n랭체인 튜토리얼,\\n문서요약,\\n질의응답,\\n크롤링\\n카테고리:\\nlangchain\\n업데이트: 2023년 10월 13일\\n참고\\n[Assistants API] Code Interpreter, Retrieval, Functions 활용법\\n2024년 02월 13일\\n35 분 소요\\nOpenAI의 LangChain 한국어 튜토리얼\\n바로가기 👀\\n랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8)\\n2023년 10월 13일\\n2 분 소요\\n이번 포스팅에서는 랭체인(LangChain) 을 활용하여 PDF 문서를 로드하고, 문서의 내용에 기반하여 질의응답(Question-Answering) 하는 방법에 대해 알아보겠습니다.\\n 후반부에는 langchain hub 에서 프롬프트를 다운로드 받고, 이를 ChatGPT 모델과 결합하여 문서에 기반한 질의응답 Chain 을 생성합니다.\\n✔️ (이전글) LangChain 튜토리얼\\n🌱 환경설정\\n🔥 PDF 기반 질의 응답(Question-Answering)\\n다음은 비구조화된 데이터를 QA 체인(Question-Answering chain) 으로 변환하는 파이프라인에 대한 기술적 번역입니다:\\n데이터 로드: 우선, 데이터를 로드해야 합니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval...\\n[LangChain] 에이전트(Agent)와 도구(tools)를 활용한 지능형 검색 시스템 구축 가이드\\n2024년 02월 09일\\n41 분 소요\\n이 글에서는 LangChain 의 Agent 프레임워크를 활용하여 복잡한 검색과 문서 기반 QA 시스템 설계 방법 - 심화편\\n2024년 02월 06일\\n23 분 소요\\nLangChain의 RAG 시스템을 통해 문서(PDF, txt, 웹페이지 등)에 대한 질문-답변을 찾는 과정을 정리하였습니다.\\n',\n",
       "  'score': 0.80735475,\n",
       "  'raw_content': '🔥알림🔥\\n① 테디노트 유튜브 -\\n구경하러 가기!\\n② LangChain 한국어 튜토리얼\\n바로가기 👀\\n랭체인(langchain) + PDF 기반 질의응답(Question-Answering) (8)\\n2023년 10월 13일\\n2 분 소요\\n이번 포스팅에서는 랭체인(LangChain) 을 활용하여 PDF 문서를 로드하고, 문서의 내용에 기반하여 질의응답(Question-Answering) 하는 방법에 대해 알아보겠습니다.\\n이번 튜토리얼에서는 langchain 의 문서 로드 - 분할 - 벡터스토어(vectorstore)에 임베딩된 문서를 저장 하는 방법을 다룹니다. 여러 벡터스토어 중 오픈소스인 Chroma DB 를 활용합니다.\\n후반부에는 langchain hub 에서 프롬프트를 다운로드 받고, 이를 ChatGPT 모델과 결합하여 문서에 기반한 질의응답 Chain 을 생성합니다.\\n✔️ (이전글) LangChain 튜토리얼\\n🌱 환경설정\\n🔥 PDF 기반 질의 응답(Question-Answering)\\n다음은 비구조화된 데이터를 QA 체인(Question-Answering chain) 으로 변환하는 파이프라인에 대한 기술적 번역입니다:\\n데이터 로드: 우선, 데이터를 로드해야 합니다. LangChain 통합 허브를 사용하여 전체 로더 세트를 둘러보세요.\\n데이터 분할: 텍스트 분할기는 문서를 지정된 크기의 분할로 나눕니다.\\n저장: 저장소(예: 종종 vectorstore)는 분할을 보관하고 종종 임베드합니다.\\n검색: 앱은 저장소에서 분할을 검색합니다(예: 종종 입력 질문과 유사한 임베딩으로).\\n생성: LLM은 질문과 검색된 데이터를 포함하는 프롬프트를 사용하여 답변을 생성합니다.\\n① 데이터 로드\\nPyPDFLoader 를 활용하여 PDF 파일을 로드 합니다.\\n② 데이터 분할\\nCharacterTextSplitter 로 chunk_size 기준으로 문서를 쪼갭니다. chunk_overlap 에 50개의 토큰을 지정하여 문서-문서 간 겹쳐지는 부분(overlap) 이 있도록 하여 비교적 유연한 요약 결과를 도출할 수 있도록 합니다.\\n③ 저장 및 검색\\nOpenAIEmbeddings 를 활용하여 문서의 내용을 임베딩한 뒤, Chroma 벡터스토어(vectorstore) 에 저장합니다.\\n마지막 줄에는 as_retriever() 로 retriever 형태로 가져오는데, 이는 추후 사용자의 query 입력시, 입력된 query로 vectorestore에서 유사성이 높은 데이터를 추출해 낼 때 쓰입니다.\\n④ 프롬프트 템플릿\\n아래의 예제는 langchain hub 에서 RAG Prompt 를 가져오는 예제입니다.\\n이처럼 langchain hub 에서 공개된 프롬프트를 다운로드 받거나, ChatPromptTemplate 를 직접 생성하는 것도 가능합니다. 자세한 사항은 ConversationChain, 템플릿 사용법 에서 확인할 수 있습니다.\\n⑤ 생성\\n마지막 단계는 LLM 모델을 정의하고 Chain 을 생성하는 단계 입니다.\\n⑥ 테스트\\n태그:\\nChatGPT,\\nChatOpenAI,\\nGPT3.5,\\nGPT4,\\nlangchain,\\nlangchain tutorial,\\nOpenAI,\\nPDF,\\n랭체인,\\n랭체인 튜토리얼,\\n문서요약,\\n질의응답,\\n크롤링\\n카테고리:\\nlangchain\\n업데이트: 2023년 10월 13일\\n참고\\n[Assistants API] Code Interpreter, Retrieval, Functions 활용법\\n2024년 02월 13일\\n35 분 소요\\nOpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval...\\n[LangChain] 에이전트(Agent)와 도구(tools)를 활용한 지능형 검색 시스템 구축 가이드\\n2024년 02월 09일\\n41 분 소요\\n이 글에서는 LangChain 의 Agent 프레임워크를 활용하여 복잡한 검색과 데이터 처리 작업을 수행하는 방법을 소개합니다. LangSmith 를 사용하여 Agent의 추론 단계를 추적합니다. Agent가 활용할 검색 도구(Tavily Search), PDF 기반 검색 리트리버...\\nLangChain RAG 파헤치기: 문서 기반 QA 시스템 설계 방법 - 심화편\\n2024년 02월 06일\\n23 분 소요\\nLangChain의 RAG 시스템을 통해 문서(PDF, txt, 웹페이지 등)에 대한 질문-답변을 찾는 과정을 정리하였습니다.\\nLangChain으로 네이버 뉴스 기반 Q&A 애플리케이션 구축하기 - 기본편\\n2024년 02월 06일\\n7 분 소요\\nLangChain을 활용하여 간단하게 네이버 뉴스기사를 바탕으로 Q&A 애플리케이션을 만드는 방법을 다룹니다.'},\n",
       " {'title': '랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1) - 테디노트',\n",
       "  'url': 'https://teddylee777.github.io/langchain/langchain-tutorial-01/',\n",
       "  'content': '태그:\\nChatGPT,\\nChatOpenAI,\\nGPT3.5,\\nGPT4,\\nlangchain,\\nlangchain tutorial,\\nOpenAI,\\n랭체인,\\n랭체인 튜토리얼\\n카테고리:\\nlangchain\\n업데이트: 2023년 09월 28일\\n참고\\n[Assistants API] Code Interpreter, Retrieval, Functions 활용법\\n2024년 02월 13일\\n35 분 소요\\nOpenAI의 temperature\\nmax_tokens\\nmodel_name: 적용 가능한 모델 리스트\\ngpt-3.5-turbo\\ngpt-3.5-turbo-0301\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k\\ngpt-3.5-turbo-16k-0613\\ngpt-3.5-turbo-instruct\\ngpt-3.5-turbo-instruct-0914\\ngpt-4\\ngpt-4-0314\\ngpt-4-0613\\n🔥 프롬프트 템플릿의 활용\\nPromptTemplate\\n사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 LangChain 한국어 튜토리얼\\n바로가기 👀\\n랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1)\\n2023년 09월 28일\\n5 분 소요\\n언어 모델을 활용한 애플리케이션 개발을 돕는 프레임워크인 랭체인(LangChain) 에 대해 깊이 있게 다뤄보고자 합니다.\\n ① run()\\nrun() 함수로 템플릿 프롬프트 실행\\n② apply()\\napply() 함수로 여러개의 입력을 한 번에 실행\\ntext 키 값으로 결과 뭉치가 반환되었음을 확인할 수 있습니다.\\n LLMChain 객체\\nLLMChain\\nLLMChain은 특정 PromptTemplate와 연결된 체인 객체를 생성합니다\\n사용법\\nprompt: 앞서 정의한 PromptTemplate 객체를 사용합니다.\\n',\n",
       "  'score': 0.7876867,\n",
       "  'raw_content': '🔥알림🔥\\n① 테디노트 유튜브 -\\n구경하러 가기!\\n② LangChain 한국어 튜토리얼\\n바로가기 👀\\n랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1)\\n2023년 09월 28일\\n5 분 소요\\n언어 모델을 활용한 애플리케이션 개발을 돕는 프레임워크인 랭체인(LangChain) 에 대해 깊이 있게 다뤄보고자 합니다.\\n튜토리얼은 시리즈 형식으로 구성되어, 시리즈를 거듭하면서 랭체인(LangChain) 을 통해 언어 모델 기반의 애플리케이션 개발은 더욱 간결하고 효과적으로 이루어질 수 있습니다.\\n🌱 랭체인의 주요 기능\\n랭체인을 통해 다음과 같은 특징을 갖는 애플리케이션을 개발할 수 있습니다.\\n랭체인의 가치\\n랭체인의 핵심적인 가치는 여러 가지가 있지만, 그 중에서도 두 가지 주요한 점을 꼽자면 다음과 같습니다.\\n특히, 이러한 사용 준비된 체인은 초보자도 랭체인을 쉽게 시작할 수 있게 도와주며, 복잡한 애플리케이션을 계획하는 전문가들은 기존 체인을 손쉽게 커스터마이징하거나 새롭게 구축할 수 있게 도와줍니다.\\n🌱 환경설정\\nAPI KEY 발급\\n먼저, openai 의 API KEY 를 발급 받아야 합니다. 발급은 다음의 절차를 통해 진행할 수 있습니다.\\nhttps://platform.openai.com/account/api-keys 로 접속합니다.\\nLog in 버튼을 클릭 후 계정에 로그인 합니다. 계정이 아직 생성되지 않은 경우에는 Sign up 으로 회원가입 후 로그인 합니다.\\n“Create new secret key” 버튼을 클릭하여 새로운 키를 발급합니다.\\nName 에는 발급하는 키에 대한 별칭을 입력합니다.\\n새롭게 발급한 키를 복사합니다. 잃어버리면 다시 발급하여야 하므로, 안전한 곳에 저장해 둡니다.\\n모듈 설치(openai, langchain)\\npip 명령어로 모듈을 설치 합니다. 아나콘다 가상환경에서 설치해도 좋습니다.\\n먼저, 설치한 openai 모듈을 import 한 뒤, 발급받은 API KEY를 다음과 같이 설정합니다.\\n사용 가능한 모델 리스트 출력\\n🔥 ChatOpenAI\\nOpenAI 사의 채팅 전용 Large Language Model(llm) 입니다.\\n객체를 생성할 때 다음을 옵션 값을 지정할 수 있습니다. 옵션에 대한 상세 설명은 다음과 같습니다.\\ntemperature\\nmax_tokens\\nmodel_name: 적용 가능한 모델 리스트\\ngpt-3.5-turbo\\ngpt-3.5-turbo-0301\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k\\ngpt-3.5-turbo-16k-0613\\ngpt-3.5-turbo-instruct\\ngpt-3.5-turbo-instruct-0914\\ngpt-4\\ngpt-4-0314\\ngpt-4-0613\\n🔥 프롬프트 템플릿의 활용\\nPromptTemplate\\n사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다\\n사용법\\ntemplate: 템플릿 문자열입니다. 이 문자열 내에서 중괄호 {}는 변수를 나타냅니다.\\ninput_variables: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.\\ninput_variables\\ninput_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.\\n사용법: 리스트 형식으로 변수 이름을 정의합니다.\\nLLMChain 객체\\nLLMChain\\nLLMChain은 특정 PromptTemplate와 연결된 체인 객체를 생성합니다\\n사용법\\nprompt: 앞서 정의한 PromptTemplate 객체를 사용합니다.\\nllm: 언어 모델을 나타내며, 이 예시에서는 이미 어딘가에서 정의된 것으로 보입니다.\\n① run()\\nrun() 함수로 템플릿 프롬프트 실행\\n② apply()\\napply() 함수로 여러개의 입력을 한 번에 실행\\ntext 키 값으로 결과 뭉치가 반환되었음을 확인할 수 있습니다.\\n이를 반복문으로 출력한다면 다음과 같습니다.\\n③ generate()\\ngenerate() 는 문자열 대신에 LLMResult를 반환하는 점을 제외하고는 apply와 유사합니다.\\nLLMResult는 토큰 사용량과 종료 이유와 같은 유용한 생성 정보를 자주 포함하고 있습니다.\\n④ 2개 이상의 변수를 템플릿 안에 정의\\n2개 이상의 변수를 적용하여 템플릿을 생성할 수 있습니다.\\n이번에는 2개 이상의 변수(input_variables) 를 활용하여 템플릿 구성을 해보겠습니다.\\n⑤ 스트리밍(streaming)\\n스트리밍 옵션은 질의에 대한 답변을 실시간으로 받을 때 유용합니다.\\n다음과 같이 streaming=True 로 설정하고 스트리밍으로 답변을 받기 위한 StreamingStdOutCallbackHandler() 을 콜백으로 지정합니다.\\n태그:\\nChatGPT,\\nChatOpenAI,\\nGPT3.5,\\nGPT4,\\nlangchain,\\nlangchain tutorial,\\nOpenAI,\\n랭체인,\\n랭체인 튜토리얼\\n카테고리:\\nlangchain\\n업데이트: 2023년 09월 28일\\n참고\\n[Assistants API] Code Interpreter, Retrieval, Functions 활용법\\n2024년 02월 13일\\n35 분 소요\\nOpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval...\\n[LangChain] 에이전트(Agent)와 도구(tools)를 활용한 지능형 검색 시스템 구축 가이드\\n2024년 02월 09일\\n41 분 소요\\n이 글에서는 LangChain 의 Agent 프레임워크를 활용하여 복잡한 검색과 데이터 처리 작업을 수행하는 방법을 소개합니다. LangSmith 를 사용하여 Agent의 추론 단계를 추적합니다. Agent가 활용할 검색 도구(Tavily Search), PDF 기반 검색 리트리버...\\nLangChain RAG 파헤치기: 문서 기반 QA 시스템 설계 방법 - 심화편\\n2024년 02월 06일\\n23 분 소요\\nLangChain의 RAG 시스템을 통해 문서(PDF, txt, 웹페이지 등)에 대한 질문-답변을 찾는 과정을 정리하였습니다.\\nLangChain으로 네이버 뉴스 기반 Q&A 애플리케이션 구축하기 - 기본편\\n2024년 02월 06일\\n7 분 소요\\nLangChain을 활용하여 간단하게 네이버 뉴스기사를 바탕으로 Q&A 애플리케이션을 만드는 방법을 다룹니다.'},\n",
       " {'title': '랭체인(langchain) + 정형데이터(CSV, Excel) - ChatGPT 기반 데이터분석 (4) - 테디노트',\n",
       "  'url': 'https://teddylee777.github.io/langchain/langchain-tutorial-04/',\n",
       "  'content': '🔥알림🔥 ① 테디노트 유튜브 - 구경하러 가기! ② LangChain 한국어 튜토리얼 바로가기 👀 ③ 랭체인 노트 무료 전자책(wikidocs) 바로가기 🙌 ④ RAG 비법노트 LangChain 강의오픈 바로가기 🙌 ⑤ 서울대 PyTorch 딥러닝 강의 바로가기 🙌. 랭체인(langchain) + 정형데이터(CSV, Excel) - ChatGPT 기반 데이터분석 (4)',\n",
       "  'score': 0.6455898,\n",
       "  'raw_content': '🔥알림🔥\\n① 테디노트 유튜브 -\\n구경하러 가기!\\n② LangChain 한국어 튜토리얼\\n바로가기 👀\\n③ 랭체인 노트 무료 전자책(wikidocs)\\n바로가기 🙌\\n④ RAG 비법노트 LangChain 강의오픈\\n바로가기 🙌\\n⑤ 서울대 PyTorch 딥러닝 강의\\n바로가기 🙌\\n랭체인(langchain) + 정형데이터(CSV, Excel) - ChatGPT 기반 데이터분석 (4)\\n2023년 10월 02일\\n4 분 소요\\n이번 포스팅에서는 랭체인(LangChain) 을 활용하여 정형데이터(CSV, Excel) 에 대한 ChatGPT 기반 질의응답을 통해 데이터 분석하는 방법 에 대해 알아보겠습니다.\\n이번 튜토리얼에서는 langchain 의 create_pandas_dataframe_agent() 을 통해 에이전트를 생성한 뒤, 생성된 에이전트에 자연어로 데이터에 대한 질의 응답 을 통해 원하는 분석 결과를 도출하는 방법에 대해 다루겠습니다.\\n한가지 흥미로운 사실은 자연어로 에이전트에 질문하면, 에이전트가 이를 pandas 문법을 변환하여 코드를 실행한 뒤, 결과를 다시 자연어로 반환해 준다는 점입니다. 에이전트가 답변을 도출하는 과정에서 실행한 pandas 코드도 확인 할 수 있습니다.\\n✔️ (이전글) LangChain 튜토리얼\\n🌱 환경설정\\n라이브러리 설치\\nOPENAI API KEY 를 설정합니다.\\n🔥 데이터 로드\\npandas를 활용하여 csv 파일을 DataFrame 으로 로드합니다.\\n🔥 Pandas DataFrame Agent\\n🔥 2개 이상의 DataFrame\\n2개 이상의 데이터프레임에 기반한 LLM 기반 질의를 할 수 있습니다. 2개 이상의 데이터프레임 입력시 [] 로 묶어주면 됩니다.\\n태그:\\nChatGPT,\\nChatOpenAI,\\ncsv,\\nexcel,\\nGPT3.5,\\nGPT4,\\nlangchain,\\nlangchain tutorial,\\nOpenAI,\\n데이터분석,\\n랭체인,\\n랭체인 튜토리얼\\n카테고리:\\nlangchain\\n업데이트: 2023년 10월 02일\\n참고\\npoetry 의 거의 모든것 (튜토리얼)\\n2024년 03월 30일\\n5 분 소요\\nPython 개발에 있어서 poetry는 매우 강력한 도구로, 프로젝트의 의존성 관리와 패키지 배포를 간소화하는 데 큰 도움을 줍니다. 지금부터 poetry 활용 튜토리얼을 살펴 보겠습니다.\\nLangGraph Retrieval Agent를 활용한 동적 문서 검색 및 처리\\n2024년 03월 06일\\n10 분 소요\\nLangGraph Retrieval Agent는 언어 처리, AI 모델 통합, 데이터베이스 관리, 그래프 기반 데이터 처리 등 다양한 기능을 제공하여 언어 기반 AI 애플리케이션 개발에 필수적인 도구입니다.\\n[Assistants API] Code Interpreter, Retrieval, Functions 활용법\\n2024년 02월 13일\\n35 분 소요\\nOpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval...\\n[LangChain] 에이전트(Agent)와 도구(tools)를 활용한 지능형 검색 시스템 구축 가이드\\n2024년 02월 09일\\n41 분 소요\\n이 글에서는 LangChain 의 Agent 프레임워크를 활용하여 복잡한 검색과 데이터 처리 작업을 수행하는 방법을 소개합니다. LangSmith 를 사용하여 Agent의 추론 단계를 추적합니다. Agent가 활용할 검색 도구(Tavily Search), PDF 기반 검색 리트리버...'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# 검색 도구 생성\n",
    "tool = TavilySearch(max_results=3)\n",
    "\n",
    "# 도구 목록에 추가\n",
    "tools = [tool]\n",
    "\n",
    "# 도구 실행\n",
    "tool.invoke(\"테디노트 랭체인 튜토리얼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# State 정의\n",
    "class State(TypedDict):\n",
    "    # list 타입에 add_messages 적용(list 에 message 추가)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOllama(model=\"phi4:latest\")\n",
    "\n",
    "# LLM 에 도구 바인딩\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# State 정의\n",
    "class State(TypedDict):\n",
    "    # list 타입에 add_messages 적용(list 에 message 추가)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOllama(model=\"phi4:latest\")\n",
    "\n",
    "# LLM 에 도구 바인딩\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노드 함수 정의\n",
    "def chatbot(state: State):\n",
    "    answer = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # 메시지 목록 반환\n",
    "    return {\"messages\": [answer]}  # 자동으로 add_messages 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1293a36d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# 상태 그래프 초기화\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 노드 추가\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1293a36d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"Run tools requested in the last AIMessage node\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        # 도구 리스트\n",
    "        self.tools_list = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        # 메시지가 존재할 경우 가장 최근 메시지 1개 추출\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "\n",
    "        # 도구 호출 결과\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            # 도구 호출 후 결과 저장\n",
    "            tool_result = self.tools_list[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "            outputs.append(\n",
    "                # 도구 호출 결과를 메시지로 저장\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(\n",
    "                        tool_result, ensure_ascii=False\n",
    "                    ),  # 도구 호출 결과를 문자열로 변환\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "# 도구 노드 생성\n",
    "tool_node = BasicToolNode(tools=[tool])\n",
    "\n",
    "# 그래프에 도구 노드 추가\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END\n",
    "\n",
    "\n",
    "def route_tools(state: State):\n",
    "    if messages := state.get(\"messages\", []):\n",
    "        # 가장 최근 AI 메시지 추출\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        # 입력 상태에 메시지가 없는 경우 예외 발생\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "\n",
    "    # AI 메시지에 도구 호출이 있는 경우 \"tools\" 반환\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        # 도구 호출이 있는 경우 \"tools\" 반환\n",
    "        return \"tools\"\n",
    "    # 도구 호출이 없는 경우 \"END\" 반환\n",
    "    return END\n",
    "\n",
    "\n",
    "# `tools_condition` 함수는 챗봇이 도구 사용을 요청하면 \"tools\"를 반환하고, 직접 응답이 가능한 경우 \"END\"를 반환\n",
    "graph_builder.add_conditional_edges(\n",
    "    source=\"chatbot\",\n",
    "    path=route_tools,\n",
    "    # route_tools 의 반환값이 \"tools\" 인 경우 \"tools\" 노드로, 그렇지 않으면 END 노드로 라우팅\n",
    "    path_map={\"tools\": \"tools\", END: END},\n",
    ")\n",
    "\n",
    "# tools > chatbot\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# START > chatbot\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "# 그래프 시각화\n",
    "visualize_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "registry.ollama.ai/library/phi4:latest does not support tools (status code: 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_teddynote\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display_message_tree\n\u001b[1;32m      3\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m테디노트 YouTube\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m==============\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mSTEP: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m==============\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1670\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1670\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1677\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langgraph/pregel/runner.py:231\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    229\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langgraph/utils/runnable.py:462\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    459\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    460\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langgraph/utils/runnable.py:226\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 226\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m, in \u001b[0;36mchatbot\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchatbot\u001b[39m(state: State):\n\u001b[0;32m----> 3\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mllm_with_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# 메시지 목록 반환\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [answer]}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langchain_core/runnables/base.py:5352\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5349\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5351\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5353\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5354\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:790\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    784\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    789\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:647\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    646\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 647\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    648\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    649\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    651\u001b[0m ]\n\u001b[1;32m    652\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:637\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 637\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:855\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langchain_ollama/chat_models.py:701\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    696\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    700\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 701\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    705\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    706\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    707\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    711\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    712\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langchain_ollama/chat_models.py:602\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    595\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    600\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    601\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mChatGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAIMessageChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/langchain_ollama/chat_models.py:589\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m chat_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_params(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 589\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/lang/lib/python3.11/site-packages/ollama/_client.py:168\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    167\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[1;32m    171\u001b[0m   part \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "\u001b[0;31mResponseError\u001b[0m: registry.ollama.ai/library/phi4:latest does not support tools (status code: 400)",
      "\u001b[0mDuring task with name 'chatbot' and id '8a46bed0-3d82-ccae-aeb3-dc2089126683'"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import display_message_tree\n",
    "\n",
    "question = \"테디노트 YouTube\"\n",
    "\n",
    "for event in graph.stream({\"messages\": [(\"user\", question)]}):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n==============\\nSTEP: {key}\\n==============\\n\")\n",
    "        display_message_tree(value[\"messages\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
